---
title: "p8105_hw5_dap2189.Rmd"
output: github_document
date: "2025-11-11"
---
```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Load additional Packages
```{r}
library(broom)
library(dplyr)
library(purrr)
```


## Problem 1: Birthday Problem

#### Step 1: Create a function 
```{r}
bday_sim = function(n_room) { #n_room = # of people i want to sample at once
  
  birthdays = sample(1:365, n_room, replace = TRUE) #possible numbers are 1 to 365. Get these values for n_room people. Allow sample to use the same number in the set

  repeated_bday = length(unique(birthdays)) < n_room #if all birthdays are unique in the set, then length of unique(bithdays) will equal # of people sampled from (n_room)

  repeated_bday #give output
  
}
```

#### Step 2: Iterate, so that we get a bunch of sample 
```{r}
bday_sim_results = 
  expand_grid(
    bdays = 2:50, 
    iter = 1:10000 #10000 values for 2 people in room, 10000 values for 3 people in room, etc.
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim) #map_lgl outputs true or false (otherwise map() gives <lgl [1]> as the output)
  ) |> 
  group_by( #groups by sample size
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result) #mean of true/false for everyone who is in the same group 
  )
```

#### Step 3: Plot Probability against # of people in Room 

```{r}
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) + 
  geom_point() + 
  geom_line()+ 
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  labs(
    title = "Birthday Paradox Simulation",
    subtitle = "When will there be a 50% chance of sharing a birthday?",
    x = "Number of People in the Room",
    y = "Probability of Shared Birthday"
  )

#currently pretty jagged when i am only iterating 50 times for each birthday
```

#### Step 4: Interpret Results 

For this simulation we have made the assumptions that there are no leap years and that someone has an equal probability of being born on any day of the year.

Based on the above simulation,  there is a near 50% chance of two people sharing a birthday when there are 22 to 23 people in a room. 

As I increased my plot from 50, 500, 1000, to 10000 iterations, the plot smoothed out extensively. 

```{r}
bday_sim_results |> 
  filter(bdays >= 20, bdays <= 25) |> 
  knitr::kable()
```

----------------------------------------------------------

## Problem 2: Detecting a True Effect
- conduct a simulation to explore power in a one-sample t-test

#### Step 1: Set up design elements
###### First set the following design elements:

```{r}
#set the following design elements
n = 30
sigma = 5
mu = 0
```

###### Generate 5000 datasets from the model:

```{r}
#x follows a normal distribution with a mean of mu and a standard deviation of sigma

#x = rnorm(n = 30, mean = 0, sd = 5) --> replace with our variables

generate_sample = function(n = 30, mu = 0, sigma = 5) {
  rnorm(n, mean = mu, sd = sigma)
}

normal_tbl = 
  tibble(
    sim_id = 1:5000,
    samples = map(1:5000, ~ generate_sample())
  )

```

###### For each dataset, save $\hat\mu$ and the p-value arising from a test of ð»:ðœ‡=0 using ð›¼=0.05 
- Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

```{r}
sample_mu_tbl = 
  normal_tbl |> 
    mutate(
      t_out  = map(samples, ~ t.test(.x, mu = 0)),    #step to run t-test on each sample
      t_tidy = map(t_out, tidy)                       #clean up using broom::tidy
    ) |>
    unnest(t_tidy) |> 
    select(sim_id, estimate, p.value)

#estimate is the sample mu
#p.value is p-valuu
#Our null is that mu = 0.


#reject or fail to reject the null hypothesis
sample_mu_tbl |> 
  mutate(
    reject_H0 = p.value < 0.05
  )
  
```

###### Repeat the above for ðœ‡={1,2,3,4,5,6}:
Time to iterate!
```{r}
test_tbl = 
  expand_grid(
    mu     = 1:6,      # true means defined as 1:6
    sim_id = 1:5000     # we want 5000 samples per mu
  )
```

###### Get p-values for all values of mu
Now run my function and get p-values for all rows in `test_tbl`
```{r}
all_iterations =
  test_tbl |>
  mutate(
    
    samples = map(mu, ~ generate_sample(n = 30, mu = .x, sigma = 5)), 
          #simulate one dataset per row, with true mean = mu. 
          #.x means the current value of mu for this specific iteration (calling on mu in b4 funct)
    
    t_out   = map(samples, ~ t.test(.x, mu = 0)), 
          #t-test of H0: mu = 0 for each dataset
    
    t_tidy  = map(t_out, tidy)
          #tidy output to get estimate and p-value
 
   ) |>
  select(mu, sim_id, t_tidy) |>
  unnest(t_tidy) |>
  mutate(
    reject_H0 = p.value < 0.05   # Î± = 0.05 decision rule
  ) |> 
  select(mu, sim_id, estimate, p.value, reject_H0)
```

###### Make a plot
- Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of ðœ‡ on the x axis. Describe the association between effect size and power.

```{r}
#Calculate proportion of times null was rejected (need to do before creating graph)

power_tbl =
  all_iterations |>
    group_by(mu) |>
      summarise(
        power = mean(reject_H0),   #probability of reject: 0 = false and 1 = true
        .groups = "drop"
      )
```

- Make the plot:
```{r}
ggplot(power_tbl, aes(x = mu, y = power)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True mean (Î¼)",
    y = "Power (proportion of rejections)",
    title = "Power Curve for One-Sample t-Test (n = 30, sigma = 5)"
  ) 
```

###### Interpretation 
As the true value of Î¼ increases, the power (probability of rejecting the null when it is false) increases at a decreasing rate. When Î¼ is small (closer to the null value of 0), the power of our test is low meaning we dont frequently reject the null (when it is false). As Î¼ becomes larger (which represents a stronger effect), our test becomes more sensitive and rejects the Hâ‚€ more often.

We see that the slope of our line plateaus as we approach 1. With a sufficiently large Î¼(ex. Î¼ â‰¥ 4 or 5), we almost always reject a false null (detecting an effect).


###### Make another plot!
- Make a plot showing the average estimate of ðœ‡Ì‚ on the y axis and the true value of ðœ‡ on the x axis.

```{r}
avg_mu_tbl =
  all_iterations |>
  group_by(mu) |>
  summarise(
    avg_estimate = mean(estimate),
    .groups = "drop"
  )
```

Make the plot:
```{r}
ggplot(avg_mu_tbl, aes(x = mu, y = avg_estimate)) +
  geom_point() +
  geom_line() +
  labs(
    x = "True mean (Î¼)",
    y = "Estimated mean",
    title = "Comparison of True Mean vs Estimated Mean"
  ) 
```

###### Overlay the plots
- Make a second plot (or overlay on the first) the average estimate of ðœ‡Ì‚ only in samples for which the null was rejected on the y axis and the true value of ðœ‡ on the x axis. 
```{r}
#Compute both averages: overall and conditional on rejection

avg_both_tbl =
  all_iterations |>
  group_by(mu) |>
  summarise(
    avg_all = mean(estimate), # average for all samples
    avg_reject = mean(estimate[reject_H0]), #the average estimate of ðœ‡Ì‚ only in samples for which the null was rejected
    .groups = "drop"
  )


#estimate[reject_H0] --> select only the values of estimate where reject_H0 is TRUE.
```

- Plot both lines on a plot:
```{r}
ggplot(avg_both_tbl, aes(x = mu)) +
  geom_point(aes(y = avg_all), color = "blue") +
  geom_line(aes(y = avg_all), color = "blue") +
  
  geom_point(aes(y = avg_reject), color = "red") + #separate by y value because we are overalying two lines
  geom_line(aes(y = avg_reject), color = "red") +
  
  labs(
    x = "True mean (Î¼)",
    y = "Average estimate (Î¼Ì‚)",
    title = "Average Estimate of Î¼Ì‚: All Samples vs. Samples Rejectingthe Null",
    subtitle = "Blue = All samples,   Red = Only samples where Hâ‚€ was rejected"
  )
```


##### Interpretation
- Is the sample average of ðœ‡Ì‚ across tests for which the null is rejected approximately equal to the true value of ðœ‡ ? Why or why not?

No, they are not always equal. When the true average is small, the mu estimator among samples that reject Hâ‚€ is larger. For tests where we reject the null, as the true value of mu increases, our estimator better approximates this value. 

In other words, we reject the null more frequently when mu is small. Only the most extreme estimates (positive or negative) are included among significant cases. As a result, the average Î¼Ì‚ among significant results is biased away from 0 and inflated relative to the true mu.

--------------------------------------------
## Problem 3: Homicides in the US

#### Step 1: Describe the raw data
Create a city_state variable (e.g. â€œBaltimore, MDâ€) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).

- Import data

```{r}
library(readr)
rawfile_url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_df = read_csv(rawfile_url)


#Alternatively, download and upload
uploaded_homicide = read_csv("Data/homicide-data.csv")

```

###### Interpretation:
The data set provides information demographic information on the victim (name, age, race, location) as well as status of the case. The below tables shows the break down for all `r nrow(homicide_df)` rows. 

```{r}

homicide_df |> 
  count(disposition, sort = TRUE) |> 
  knitr::kable()
```


- Create a city_state variable (e.g. â€œBaltimore, MDâ€) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is â€œClosed without arrestâ€ or â€œOpen/No arrestâ€).

```{r}
city_stats =
  homicide_df |> 
    mutate(city_state = paste(city, state, sep = ", ")) |> 
    group_by(city_state) |> 
    summarise(
      total_homicides = n(),
      unsolved_homicides = sum(disposition %in% c("Closed without arrest","Open/No arrest")),
      .groups = "drop"
      #percent_unsolved = (unsolved_homicides / total_homicides)*100
    ) |> 
    arrange(desc(total_homicides))
```

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
baltimore_stats = 
  city_stats |> 
  filter(city_state == "Baltimore, MD")

baltimore_stats


```


```{r}
baltimore_prop =  
prop.test(
  x = baltimore_stats$unsolved_homicides,  # number unsolved
  n = baltimore_stats$total_homicides      # total homicides
)

#I couldnt use a pipe (and am using $ instead) because prop.test wouldnt allow it....)
```

- Tidy with Broom
```{r}
baltimore_prop = tidy(baltimore_prop)
baltimore_prop

prop_estimate =
  baltimore_prop|> 
  pull(estimate)

ci_low = 
  baltimore_prop |> 
  pull(conf.low)

ci_high = 
  baltimore_prop |> 
  pull(conf.high)
```

For Baltimore, we get a proportion estimate of `r prop_estimate` with a 95% confidence interval of (`rci_low`,`r ci_high`). 

#### Do it for all cities
```{r}
city_props =  
  city_stats |> 
  mutate(
    prop_test = map2(unsolved_homicides, total_homicides, ~ prop.test(.x, .y)), # list-column of htest objects
    prop_tidy = map(prop_test, broom::tidy) # list-column of 1-row tibbles
  ) |> 
  unnest(prop_tidy) |> 
  select(city_state, total_homicides, unsolved_homicides, estimate, conf.low, conf.high)
```

#### Create a plot

```{r}
city_props |> 
  mutate(
    city_state = fct_reorder(city_state, estimate) # so that cities are ordered by unsolved prop (easier to read)
  ) |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) + #error bars become size of my 95% CI
  coord_flip() +
  labs(
    x = "",
    y = "Proportion of Unsolved Homicides",
    title = "Estimated Proportion of Unsolved Homicides by City",
    subtitle = "Error bars = Upper and Lower Bounds of the 95% Confidence Intervals"
  ) 
```

###### Interpretation
- Chicago, IL has the highest proprotion of unsolved homicides with the narrowest confidence interval (73.59%, 95%CI 0.7240 - 0.7474). Tulsa, AL has a zero proprtion (with the widest confidence interval). When we look at the data, we see that Tulsa has only one homicide and has no unsolved homicides. Richmond, VA has the actual lowest proportion of unsolved homicides. Out of the 429 total homicides, 113 are unsolved (26.34%).




